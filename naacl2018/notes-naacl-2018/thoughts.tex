\section{What do people work on?}
NAACL 2018 is the first ML / NLP conference I go to (went to ICRA 2017 but that was robotics), and I am looking for directions for future thesis / projects / etc., so this is a natural question to ask. In general, their topics vary, but some common trends could be observed.  

First trend is \textbf{new progresses on traditional tasks} (sequential prediction, summarization, comprehension, translation, question-answering, NER, etc.) Word embedding, for example, is another direction that is continuously improved upon. There are word embeddings that incorporate language models (the best paper this year), incorporate prepositions, etc.

Second trend is the emerging of \textbf{new tasks, inspired by real-world applications}. Data-efficient learning (few-shot learning, semi-supervised learning, etc.) algorithms have been applied, for example. Technologies relating to health (e.g., EHR records, CLPsych) and society (fairness, law, etc.) are also mentioned. In business, Alexa and Google Assistant require and inspire advances in conversational agents. Text mining from travel / hotel / product reviews inspire some very interesting works.  

Third is \textbf{new tasks brought in by the advance of core machine learning}. CNNs (even Transformers) have started to take the place of RNN for sequential tagging tasks. Many domain adaptation papers begin to use shared-private layers and adversarial loss. Reinforcement Learning are becoming popular as well.  

Fourth trend is the increased focus on \textbf{developing explainable models and evaluation metrics}. There was a talk on the stability of word embeddings, several posters addressing the evaluation metrics. To make the models more explainable, some tools in information theory are applied (e.g: information bottleneck for topic modeling in TACL).  