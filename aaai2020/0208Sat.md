## W11S: Evaluating Evaluation of AI systems

[The eval.how website with all links to their papers](http://eval.how/aaai-2020/program.html) 

930-1030 Anna Rogers: The questions that the current AI can't answer

- Models could still give good performances when inputting random embeddings.
- BERT etc., model performances drop when we paraphrase.
- Human evaluation results largely depend on the questions.

11-12 Paper session 1

Jose Hernandez-Orallo: [AI Evaluation: On Broken Yardsticks and measurement scales](http://eval.how/aaai-2020/REAIS19_p14.pdf)

- AI measurement suffers from a moving-target phenomenon.
  - A "challenge-solve-and-replace" dynamics of AI benchmarks.

- The moving target: five possible causes
  - AI effect: whenever something is automated, it is not intelligence any more
  - Superhuman abyss: What does it mean by saying "it goes beyond human performance"? There are many unfair extensions.
  - Resource neglect: breakthroughs are obtained with huge resources in terms of data, compute, supervision and other
  - Specialisation drift. Tendency of AI researchs to specialise to a particular task, or to overfit to a benchmark
  - Cognitive judge problem: manual or automatic cognitive effort is needed to produce and verify instances, frequently relying on human labellers or crowdsourcing.
- How to improve the evaluation metrics? Divide tasks into different categories:
  - Ceiling. Ground truth is human intelligence. E.g., Turing Test, human voice generation.
  - Projectional. Once AI reaches human performance, the projected score is meaningful.
  - Transitional. Add dimensions once human performance is reached. E.g., Add noise to ImageNet.
  - Universal extrapolation. E.g., Predict brain tumor 5 years before.
- For each task, the measurement metrics (Table 1).
- Could also measure from multiple dimensions.

Christopher Pereyda and Lawrence Holder: [Measuring the relative similarity and difficulty between AI benchmark problems](http://eval.how/aaai-2020/REAIS19_p8.pdf)

- A similarity framework to describe task similarity.
- Evaluate this framework, by DNN performances on foundational datasets (Classification: MNIST, Fashion-MNIST, C10, C100, CP. RL environments: misc.).

Azamat Kamzin, Prajwal Paudyal, Ayan Banerjee and Sandeep K.S.Gupta: [Evaluating the gap between hype and performance of AI systems](http://eval.how/aaai-2020/REAIS19_p13.pdf)

- First look at the task, and then some up with evaluation metrics.

- Five best practices for evaluating AI systems that can potentially address and explain the gap between hype and practically observed performance of AI systems.
  - BP1: Transparency on the effects of evaluation methodologies on participant bias
  - BP2: Disclosure of priorities of evaluation criteria
  - BP3: Choosing robust metrics
  - BP4: Evaluation in terms of explainable concepts
  - BP5: Iterative evaluation on adaptive human AI interaction

Chris Welty, Praveen Paritosh, Lora Aroyo: [Metrology for AI: From benchmarks to instruments](https://arxiv.org/abs/1911.01875)

- Would you use a yardstick to measure the thickness of a piece of paper?
- 35% of empirical paper authors don't believe error bars to be important.
- Learn from the metrology community.
  - Reproducibility is very important. Reproducibility -> Repeatability.

12-13 Odd Erik Gunderson: How can we know it is shoulders we stand on? Reproducibility and evaluation



14-1430 Paper session 2

Botty Dimanov, Umang Bhatt, Mateja Jamnik and Adrian Weller: [You shouldn't Trust Me: Learning models which conceal unfiarness from multiple explanation methods](http://eval.how/aaai-2020/REAIS19_p7.pdf)

- Message: feature importance contain little information about model fairness
- Motivating example: while the importance of features are unchanged, a modified model could give maximal violation of demographic parity.
- Propose a method to modify the classifiers..

Deborah Raji and Genevieve Fried: [About Face: Tracking shifting trends of facial recognition evaluation](http://eval.how/aaai-2020/REAIS19_p12.pdf)

- Facial recognition + fairness

- Different era has different data source + technology + demographic information

  

1430-1530 Sam Henry. Amazon Augmented AI: People + AI = Magic

- Background
  - Humans continuously review for ML auditoring
  - Combining humans and ML is hard
- Product: Amazon Augmented AI (A2I)
  - Easily implement human review of machine learning predictions
- BTW: $4000 funding for collecting datasets.



1600-1645 Paper Session 3

Stefano Alletto, Shenyang Huang, Vincent Francois-Lavet, Yohei Nakata and Guillaume Rabusseau: RandomNet: Towards fully automatic neural architecture design for multimodal learning

Julian Niedermeier, Goncalo Mordido and Christoph Meinel: Improving the evaluation of generative models with fuzzy logic

Huiyuan Xie, Tom Sherborne, Alexander Kuhnie and Ann Copestake: Going beneath the surface: Evaluating image captioning for grammaticality, trustfulness, and diversity



1645-1730: Panel: perspectives on self-evaluation. Should researchers, goups, companies evaluate their own work, or is a more adversarial approach necessary to achieve evaluation quality?