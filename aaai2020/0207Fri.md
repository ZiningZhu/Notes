## Tutorial FA1: AI in Precision Medicine

- Instead of interpreting models in a black-box manner, we should build explainable models in medication.

## Tutorial XAI

Their slides are at the website [xaitutorial2020.github.io](https://xaitutorial2020.github.io/)

## Workshop PPAI

[Privately computing influence in regression models](https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/papers/paper_25.pdf)

- Compute an influence score that encourages participants to give higher quality data points.
- The influence score is not computable: approximate it.
- TODO - figure link

[Private learning for high dimensional targets with PATE](https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/papers/paper_29.pdf)

- PATE is for classification. How about image segmentation?
- Approach: PCA to lower dimensions, then you can do PATE.
- Details: Need to empirically determine the dimension for PCA. Also, need to bound the activations. Used an activation function that projects the logits onto unit ball.
- TODO - figure link

[Plans that remain private, even in hindsight](https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/papers/paper_27.pdf)

- Want to protect robot information from the maintainer (adversary).
- Problem formulation: described a hierarchy of the maintainer's knowledge (weakest: know nothing; highest: know the exact plan)
- Method:
  - Use bipartite graph (world state -> plan state) to represent the problems.
  - Use rule-based methods (formal systems) to protect information.
- TODO - figure link

[A survey of differentially private GANs](https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/papers/paper_9.pdf)

- TODO - figure link

[Scalable and provably accurate algorithms for differentially private, distributed decision tree learning](https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/papers/paper_12.pdf) 

- Decision Tree has a proof that the accuracy in each split operation does not become worse. Try to give a similar guarantee for differential privacy.
- Also gave a comparison of three approaches empirically.
  - NoisyCounts
  - LocalRNM

## Tutorial FP1: Differential Deep Learning on Graphs 

Chengxi Zang. [Tutorial website](http://www.calvinzang.com/DDLG_AAAI_2020.html)

**1. Introduction: differential deep learning and their graph applications**

Graph applications example:

- Network dynamics of social media. 
- Traffic data.

Examples of DNN and differential equations:

- Residual Net -> Differential equations -> Neureal ODE

- RNN -> Residual RNN -> differential equation RNN

Example of normalizing flow -> diff equations

- A normalizing flow is an invertible generative model.
- P(X) -> P(Z): inference. P(Z)->P(X): generation
- Flow -> residual flow -> differential equation flow.

NICE (neural invertible flow) 

- NICE comes from Hamiltonian Systems
- NICE vs RealNVP
- NICE vs differential equations.

Then we could do the other way. Converting diff equations into DNNs by numerical methods.



**2. Molecular graph generation in drug discovery** 

 Encoding graph is hard. Decoding graph is much harder. (e.g., chemical constraints).



**3.Learning dynamics on graphs**

Goal: to predict temporal change or final states of complex systems

- Use differential equations (parameterized by neural networks) to model the system dynamics.

Three tasks:

- Continuous-time dynamics prediction
- Structured sequence prediction
- Node classification / regression

Propose to use NDCN (neural dynamics on complex networks) as a unified framework to solve all three of them.

**4. Mechanism discovery in graphs**

Goal: to find dynamical laws of complex systems

**5. Conclusion: discussions and Future Directions** 
