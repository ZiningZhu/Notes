# 0604 Tue  

## Morning poster
Modeling document-level causal structures for event causal relation identification  
![Poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093715/Blog/2019_naacl/0604poster_causal_aee3oc.jpg)  

Text similarity estimation based on word embeddings and matrix norms for targeted marketing
![Poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093699/Blog/2019_naacl/0604poster_embedding_raxlho.jpg) 
- Why normalize over the matrix norms? Because you want to collapse out those frequent words while preserving those with the largest eigenvalues. 

## 5A: Multilingual NLP  
(11-12:30, Nicollet D)  

Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus  
- Dataset:  
  - DementiaBank: 551 samples  
  - Lu corpus: 49 picture description samples. Not enough data here.  
- Idea: extract features separately, learn find correspondences from out-of-domain data!   
- Movie subtitles: OpenSubtitles corpus contain aligned subtitles in 62 languages.  
- Proposed model: feature transfer model trained on OpenSubtitles + English features model (previous work).  
- Evaluation: mandarin features -> English features (using the feature transfer model), then use the previous work models.  
- How to do feature transfer?  
  First select the top-k features, ordered by $R^2$.  
  Then take the corresponding features.  
- Plot: number of best features and the resulting evaluation results (Spearman rho, with the predicted scores)  
- Ablation study: how many parallel sentences are needed?  
- Summary: we can leverage an out-of-domain parallel corpus to detect dementia in Mandarin Chinese.

Cross-lingual visual verb sense disambiguation    
- Task: word sense disambiguation with images.  
- Crosslingual visual sense disambiguation
  - visual model: ResNet features  
  - textual model: avg of word2vec embeddings as features  
  - multimodal model: merge visual and texual features using early fusion   
- Crosslingual VSD results
- Example of the results
- Evaluation: multisense dataset with image descriptions  
- Crosslingual VSD and MT examples
- Summary: can use imagees to disambiguate senses across languages

Subword-level language identification for intra-word code-switching  
- Task: classify the languages (language identification, LID)  
- When the words consist of morphemes from multiple languages, code-switching is more common when languages are morphologically rich.  
- Model: end-to-end model based on segmentational RNN.  

MUST-C: a Multilingual speech translation corpus  
- Motivation: paradigm shift in MT  
- Data source: TED talks  

Contextualization of morphological inflection  
- What is morphological inflection?  
- Task: predict the fully inflected sentence from its partially inflected version.  
- Model: Neural CRF with an RNN potential.  
- Experiment data: universal dependencies v1.2
- Baseline: SIGMORPH, BiLSTM (Cotterell et al., 2018) 

A robust abstractive system for cross-lingual summarization  
- Problem: summarization
- Approach:
  - Train a baseline abstractive summarization system on NYT (English) annotated corpus
  - Automatically translate NYT articles into a low-resource language and back to (noisy) English 
  - Pair the synthatic "translations" with the clean English NYT reference summaries 
  - Train robust summarization systems using these pairs.
- Baseline abstractive summarizer: pointer-generator network with coverage vector
- Machine translation: Marian framework. The low-resource languages are Somali, Swahili, and Tagalog.
- Summarization: seq2seq with copy-attention and coverage.
  - Pretrain on unmodified NYT (12 epochs)
  - Train 8 epochs on "noisy" NYT  
- Evaluation on ROUGE-1, ROUGE-2, and ROUGE-L
  - Ssanity check on a held-out test set of 6k "translations" from each language
  - Real world evaluation: 20 weblog entries from each language. Use 5 human evaluations per article. Score on content and fluecy etc..  

## Keynote 2: When the computers spot the lie and people don't 
(2-3pm, Nicollet Ballroom. Prof Rada Mihalcea)  

1. Deception precedes language  
  "The ultimate goal of learning fo speak might be lying" (Jean Aitchison 1996)  
2. Deception is very frequent (around 2 lies per day. lol)  
3. Deception is very diverse  
   Diverse reason for lying.
4. Humans are poor lie detectors  
-> We should look at the difference between behavioral and true clues.   
4.1 Standatd data annotation strategies do not work  
4.2 Deception detection is not a big data problem  

5. Deception (detection) is central to many domains  

6. Deception (detection) has many social implications  
- Ethics and bias
- Are they ready for massive use? Need large-scale experiments.
- Misinformation


## 6D: Question answering  
(3:30 - 5pm, Nicollet B/C)  

Improving machine reading comprehension with general reading strategies  
- Focus on several strategies: 
  - Go back and forth in the text to find relationships among ideas
  - Highlight information in the text
  - Ask myself questions I would like to have answered in the text, then check to see if my guesses about the text are right or wrong.
- Back & Forth reading: consider both the original and reverse order of an input sequence. (Fine-tune two pre-trained LMs, but the token-level sequneces within each sentences are preserved.)  
- Highlighting: add a trainable embedding during fine-tuning (basically giving it a special emb value if the POS belongs to the "content words" set).
- Ask myself questions: self-assessment    
- Evaluation: RACE (reading comprehension Q&A dataset)    
- Open-sourced the codes.

Multi-task learning with sample re-weighting for machine reading comprehension  
- Problem: Machine Reading Comprehension (MRC) do not have too much data.
- Previous approaches to augment data: language modeling, etc.
- Propose method: multi-task learning. Multi-Task Stochastic Answer Network  
- MLT training algorithms: 
  - Basic training: randomly pick dataset, then randomly shuffle minibatches. Problem: negative transfer.
  - Mixture Ratio. In each iteration, randomly ly pick a fraction of external dataset and add to the chosen dataset.
  - MTL with sample re-weighting.  
- Evaluation on SQuAD and NewsQA.
- Open-sourced the codes.


Iterative search for weakly supervised semantic parsing  
- Presentation: "this work in one slide" summary.  
- Task: semantic parsing: translating utterances into program-executable sentences.  
- Training semantic parsing with denotation-only supervision is challenging because of spuriousness: incorrect logical forms can yield correct denotations.
- Two solutions:  
  - Iterative training  
  - Coverage during online search. Coverage is a measure of relevance of the candidate logical form $y_i$ to the input $x_i$, in terms of how well the productions in $y_i$ map to parts of $x_i$.    
- Got SOTA single model performances: WikiTableQuestions with comparable supervision, and NLVR semantic parsing with significantly less superivsion.  


Alignment over heterogeneous embedings for QA
- Task: QA (multiple choice)  
- AHE architecture:
  - Aligns each word in question and answer to its most similar word in the supporting paragraphs.  
  - The supporting paragraph is retrieved using an off-the-shelf IR tool
- Embeddings considered: FLAIR (char-based), BERT (word-based), InferSent (sentence-based). Ensemble the results from all three embeddings.  
- Evaluation: ARC (AllenAI's reading comprehension), WikiQA
- Further analysis: (1) How complementary are the representations? (2) How well does the model transfer between domains? (3) Manually analyze some questions that AHE model gave incorrect results. 