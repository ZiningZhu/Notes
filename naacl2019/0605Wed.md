# 0605 Wed 

## Morning keynote: Building NLP applicaitons that real people can use
(9-10, Nicollet Ballroom)  
About textio.com:  
- Your language reflects your culture.  

## 7F: Posters
(10:30-12:00 Exhibit Hall)  

Segmentation-free compositional n-gram embedding  
- Given a sentence, look at all its subword components, and then output one embedding vector.
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093703/Blog/2019_naacl/0605poster_04_o80blu.jpg)

Misspelling-oblivious word embeddings  
- Based on FastText, but add the spelling correction loss $L_{SC}$.
- FastText (word2vec) loss encourages word & subwords within the context of each other to stay closer.
-  $L_{SC}$ encourages tokens and their common misspells to stay close to each other.
-  How to generate mis-spelled words? Mine the query search logs of facebook.
-  Evaluation: intrinsic (similarity and analogy) & extrinsic (POS tagging ). The misspelled words are also mined from search engines.
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093709/Blog/2019_naacl/0605poster_06_c5rzj0.jpg)

Understanding learning dynamics of LMs with SVCCA  
- SVCCA = SVD (to reduce latent vectors dimensions) + CCA (to check the similarity between LSTM LM and a tagger)  
- Intermediate layers become similar to a tagger during training, but the embedding layers are different.
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093704/Blog/2019_naacl/0605poster_09_ceonlh.jpg)

Continual learning for sentence representations using conceptors
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093700/Blog/2019_naacl/0605poster_11_ibsocz.jpg)

Investigating robustness and interpretability of link prediction via adversarial modifications  
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093707/Blog/2019_naacl/0605poster_17_vazex6.jpg)

Analysis methods in NLP: a survey  
- Visualization
- Finding linguistic information in neural models
- Finding challending sets
- Adversarial examples
- Explaination
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093706/Blog/2019_naacl/0605poster_18_jovrjf.jpg)

Gating mechanisms for combining char and word-level word representations: en empirical study  
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093710/Blog/2019_naacl/0605poster_22_r5ndla.jpg)

Neural text normalization with sub-word units  
![poster](https://res.cloudinary.com/dnijsrvoc/image/upload/v1560093708/Blog/2019_naacl/0605poster_31_zqkire.jpg)

## 8E: Bio & Clinical 
(1:30-3pm Nicollet D)  

Multilingual prediction of Alzheimer's disease through domain adaptation and concept-based language modelling  
- Transfer results from English AD detection dataset into French (much smaller)  
- Identify a set of transferrable features
  - Information content units
  - info features
  - LM-derived features

Ranking and selecting multi-hop knowledge paths to better predict human needs   
- Research in sentiment analysis  
- Show that integrating commonsense knowledge help the model to better predict human needs.
- Contributions:
  - Selecting and ranking multi-hop relation paths from a commonsense knowledge resource.
  - end-to-end model enhanced with self-attention and a gated knowledge integration component to predict human needs.
  - Knowledge paths provide interpretability.


NLP whack-a-mole: challenges in cross-domain temporal expression extraction  
- Goal: 
  - evaluate performance of Chrono on a clinical corpus (Newswire dataset) before and after algorithm improvements.
  - Identify parsing issues.
- Background: temporal resolution: extracting, normalizing, and interpreting temporal information in text.
  - Subtasks: (Chrono focuses on) temporal expression identification  
- Temporal representation: SCATE vs TimeML
- Evaluation: 
  - Precision, recall, F1
  - 100% vs span-only
- Sumary:
  - Identify 6 domain shift challenges:
    - Ditribution of entity types
    - Lexical diversity
    - Frequent frequency
    - Disambiguating dosage
    - Cross-domain ML training data
    - Lexical variation
    - Document structure
- Chrono's web interface: [here](https://chronocirrus.com)
- Slides [here](http://bit.ly/nlpwhackamole)


Document-level N-ary relation extraction with multiscale representation learning  
- Task: N-ary relation extraction  
- Goal: assist expert curators.
- Attempt 1: ...? 
- Attempt 2: Binary decomposition
- Attempt 3: Full document model
  - Model scores every triple of (drug, gene, variant) in the document
- Multiscale combination


Inferring which medical treatments work from reports of clinical trials
- Prediction task: Given a prompt and an article, produce an answer and the rationale.  
- Dataset: Full-text articles describing RCTs.
- Present a new corpus (this above dataset).
- Website [here](http://evidence-inference.ebm-nlp.com)


## 9D: Cognitive & Psycholinguistics
(3:30-4:30pm Nicollet D)

SNAP-BATNET: Cascading author profiling and social network graphs for suicide ideation detection on social media  
- Created a large dataset  
- Motivation: suicide as a social problem. The correlation between social media use and suicidal. 
- Dataset: keyword; author profiling; feature design and extraction (e.g., TF-IDF, POS counts, NRC emotion, GloVe embedding, LDA); 
- Text-based models
- Author-based models
- Combining social graphs
- Final model: SNAP-BATNET. Used a meta-learner to combine the predictions made using each feature set. Useful in combining feature sets of different distributions.

A large-scale study of the effects of word frequency and predictability in naturalistic reading  
- Goal: show if there are separable effects of frequency and predictability in naturalistic reading. 
- Naturalistic reading: ...?  
- Frequency is depicted by unigram log probability  
- Predictability is characterized by 5-gram surprisal  
- Result: either of frequency and predictability affects naturalistic reading, but there is no evidence of separable effects. 

Augmenting word2vec with LDA within a clinical application  
- Add LDA information into linguistic feature vectors
- And then classify dementia

On the idiosyncrasies of the Mandarin Chinese classifier system  
- Classifiers encode an oncology. e.g., "支"，“块”，“条”. (This is not the 'classifier' in ML classifiers)    
- "Classifier system display many of the imaginative aspects of mind, especially the use of mental images
 (Lakoff, 1987)  
 - Question: how idiosyncratic is the Mandarin chinese classifier system?  
 - Method: compute the mutual information between the classifiers and nouns (expect this MI to be highest), and classifiers and adjectives (should be >0), and I(Classifiers; Synsets - This are lower than the previous two).   
 - Dataset: Chinese Gigaword corpus
 - Parse: Google's pretrained Parsey Universal. Then obtain empirical distributions of classifiers, nouns and adjectives.    

(another session) A structural probe for finding syntax in word representations  
- Want to test whether syntax trees are encoded in linear projections of word embeddings vectors.
- The linear probe model (parameterized by the matrix $B$ in the paper): given a sequence of words, output a sequence of vectors. 
- To optimize, let the (L1) pairwise distance between word vectors go towards the tree distance between words in syntax trees.
  - Distance between word vector: $(B(h_i - h_j)^T (B(h_i - h_j)))$
  - Tree distance within words syntax trees: For neighbors, $d(u,v)=1$. 
  - Also considered parse depths besides tree distance.
- How to tell if an embedding has encoded the tree structures? 
  -  For tree distance: Undirected Unlabeled Attachment Score (UUAS), avg Spearman correlation.  
  -  For parse depth: root prediction accuracy and avg Spearman correlation.  

## Best paper session
(4:45 - 6:15 Nicollet Ballroom)

CNM: An interpretable complex-valued network for matching  
(Best explainable paper. Li et al)  
- Research problem: interpretability issue for NN-based NLP models. According (Lipton '16):  
  - Transparency: explainable component in the design phase
  - Post-hoc explainability: why the model works after execution  
- Motivation: Quantum Theory is beneficial to model uncertainty.
- Related work: Existing quantum-inspired models for NLP and IR tasks.
- Proposal:
  - A unified quantum view of different levels of linguistic units
    - Sememes
    - Words
    - Word combinations
    - Sentences
  - An end-to-end complex-valued network
    - Numerically constrained components
    - Explainable as quantum concepts
- Quantum preliminaries:
  - Hilbert space: inf dimensional.
  - Pure state: basic state / superposition state. Mixed state: to describe a set of quantum particles.
  - Measurement
- Uncertainty in language vs. uncertainty in QT
  - Single word <-> uncertainty of a pure state
  - Multiple words <-> uncertainty of a mixed state
  - High-level semantic models <-> Hilbert Space features 
- Application to text matching.  
- CNM: network structures
- Experiment datasets: TREC and WikiQA: select the most similar candidate answer given a question.
- Post-hoc explainability

CommonsenseQA: A QA challenge targeting commonsense knowledge  
(Best resource. Talmor et al)  
- Common sense in NLP example: Winograd Schema challenge.  
- Problem: reporting bias
  - Conversational maxim of quantity: communication should be as informative as necessary but no more than required (Grice's maxim)  
- Possible solutions
  - Human annotation
  - Machine implicitly learning
- Goal: 
  - Generate a challenging dataset for commonsense reasoning.
- How to avoid human bias? Solution: use ConceptNet as a "seed", then let human annotators do multiple choice questions.
- Contributions:
  - A new method for generating commonsense QA dataset.
- Experiments and baselines
  - VecSim
  - ESIM (Chen et al., 2016)
  - BIDAF++
  - GPT / BERT: sentence [sep] answer -> one logit.   
  - Human performance. Much better than all previous models.  
- CrowdSence: [Website](tau-nlp.org/commonsenseqa) and [Demo](crowdsense.apps.allenai.org)

Probing the need for visual context in multimodal MT  
(Best short paper. Caglayan et al)  
- Task: multimodal machine translation
- Dataset: Multi30k. 
- Potential benefit:
  - Language grounding (sense disambiguation, grammatical gender disambiguation, new concepts)  
- Images doesn't help yet. In this paper we show this is because the Multi30k dataset is too simple.
- We degrade source language (e.g., color masking, entity masking, progressive masking)  
  - Hypothesis 1: MMT models should be better than text-only models if image is effectively taken into account.  
  - Hypothesis 2: more sophisticated MMT models should perform better than simple MMT models.  
- Experiments: the effects of masking (color / entity), progressive masking. Also compared attentive MMT vs. simple INIT grounding.

BERT: pretraining of deep bidirectional transformers for language understanding  
(Best long paper. Devlin et al)  
- Pre-training task: language modeling
- Downstream task: fine-tune on various language understanding problems
- Model structure: multi-layer bidirectional model, which is the Encoder of Transformer. Note: in the Vaswani 2017 paper, their "Transformer" is an encoder and a decoder. BERT only takes the encoder component.  

What's in a name? Reducing bias in bios without access to protected attributes  
(Best thematic paper. Romanov et al)  
- Task: classify occupation from the people's online bibliography, but discourage the correlation between the prediction and a word embedding of their name.  
- Method: add a constrained loss to the optimization target: either of cluster CL or covariance CL.
  - Cluster CL: average pairwise distance of the cluster centroids of name vectors (embeddings of names).  
  - Covariance CL: the covariance between prediction of a data point and the individual name vector.
- Evaluation:
  - Datasets: Adult income dataset from UCI, and Bios dataset (De-Arteaga et al, 2019)
  - Quantifying bias: true positive rate (TPR) race gap and TPR gender gap.