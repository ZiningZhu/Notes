# 0602 Sun  

## Morning Tutorial: measuring and modeling language change  
Jacob Einstein  
Slides and notebooks at [github](https://github.com/jacobeisenstein/language-change-tutorial)  

### 1. Motivations  
Observe language changes at long / short time scales.  
What changed?  
- Systemic change. e.g., in grammars  
- Usage change: what people decided to talk about  

Weinreich, Labox, and Herzog (1968) present five problems: 
- constraints
- transition
- embedding (what implications does a change have for the larger linguistic system?)
- evlauation (social meaning of a particular change?)
- actuation (why this change, and why now?)  

Sources of information about language change
- corpora
- lexicons: list of word types or cognate sets
- simulation: simple models and their ability to explain observed phenomena  
- apparent time: differences between individuals by age

Language changes beyond linguistics  
- computational soical sciences (text-as-data)  
- digital humanities (distant reading)  
- human-computer interaction (social computing)  

Most of these fields are interested in explanation, not prediction.  
- NLP can play an important role by operationalizing variables of interest  
- Rigorously evaluating explanations is different and usually harder than evaluating predictions, and requires a different way of thinkinb.  

### 2. Five practical methods and case studies  

**2.1 Word frequency**  
Culturomics: quantitative analysis of culture using milliosn of digitized book (Google 2011?)  
Plot the frequencies of words:  
- Which words?  
- Which texts?
- What to count?
- Word frequencies and events have timestamps, but did one really cause the other?  

(Liberman 2010): "s" vs. "f" in OCR -> might be getting the wrong string from Google Books.  

Composition effects:  
- Scientific aticles occupy an increasing proportion of Google Books over time.  

Artisanal corpora  
- Corpus of Historical American English (COHA)
- British ...

Case study 1: Golder and Macy 2011: Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures.  
- Main question: how does mood shift across ths day, week, and season?
- Text: positive and negative affect lexicons from LIWC.  


**2.2 Difference**  
Questions including:  
- Is political polarization increasing?
- Is polarizing becoming more about identity groups than policy ideas?
- Are gender roles becoming less stereotypical in fiction?  

Quantifying differences:
- Measure word frequencies. Relies fairly directly on the original data, but word frequencies are high-variance and not really IID.  
- Latent topics. Aggregates related words, but may depend on idiosyncrasies of prepocessing and hyperparameters.  
- Classifier accuracies. Powerful classifiers can relax IID assumption, but are computationally expensive and sensitive to dataset design.  

Information theoretic measures of difference.  
- Entropy: how oncentrated is probability mass over words / topics. 

**2.3 Word meanings**  
Change in the lexicon
- Changes in a corpus may be driven by new real-world events and entities (e.g., email, tablet)
- Linguistic 'fashions' involve new signs for existing meanings (e.g., lol)
- ...

Meaning change in distributional semantics
- Hypothesis: if the meaning of word w is summarized by a distributional representation $u_M$. Then the change in meaning should be reflected by changes in $u_M$.  

Properties in word embeddings:  
- Fixed-lengthe vector
- Similar words appear with large cosine similarity 

**2.4 Leaders and Followers**  
- How does a change propagate in a community of speakers? More specifically, who leads? Who follows? Who resists?  
- Estimating influence from diachronic data. E.g., Kalman filter / smoother, generalized Kalman, Dynamic topic model, Gaussian Process, multivariate Kawkes processes, point process models

**2.5 Cause and effect**  
- Text as cause, text as sequence.
- Gold standard: randomized control trial (RCT), but are expensive, unethical, and (most likely) impossible in the large-scale social experiment scenario.
- Model: Granger causation of function shifts

### 3. Next steps
Elaborate the connection between lexical change and other forms of systemic language change.
  - Contrasting lexical and phonological change
  - Measuring syntactic change: specific constructions, POS trigrams
  - How good is syntactic analysis on historical text?
Make NLP systems more robust to language change, without labeling thousands of documents.
  - Construct validity challenges
  - Other collaborative models
Allow non-specialists in the humanties and social sciences to tell NLP systems what they want without labeling thousands of exmaples.


## Tutorial: NLI  
Slides: [this site](https://nlitutorial.github.io/nli_tutorial.pdf)  
## Tutorial: Adversarial Learning
Slides: [google drive](https://drive.google.com/drive/folders/1E4uHe4_TD4yDJws3t1kXJQanUFJiqpBB)

## Tutorial: Transfer learning in NLP
Slides: [google slides](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit)  

## Tutorial: Language learning and processing in people and machines

**Human lanauge acquisition**  
How do children learn language? (from a lot of noisy and ambiguous inputs)  

1. Is language learned? How?  
Empiricism vs. Nativism  
Behaviorism vs. Cognitivism  
Domain-general vs. Domain-specific learning  
Functionalism vs. Formalism (language for communication)

2. Is language learning effortless?  
Takes children 5 years (14,600h if 8 hrs per day)  
Takeaways: should AI models make the same mistakes as children? Should we model all the domains of learning at the same time?  

3. Learning mechanisms.  
- Babies as statistical learners (Saffran et al, Science 1996: children can determine word boundaries)  
Statistical learning in other domains  
- Babies as rule learners.  
- Babies as social learners.

4. learning about words  
   
5. Learning the systems
- Language is productive (both syntactic and morphological)  
- Syntax: leavel of abstraction
  e.g., "Rita drinks milk". "agent -> action -> theme" is one level of abstraction. But "Rita resembles Ray" does not fit in this level of abstraction. Another level can be "noun -> verb -> noun".  
- Syntax: type of structure. Is human language use even hierarchical? Or are they just sequential?  
- Morphology. Children learn morphology earlier when language is morphologically righ (Peters, 1995) Easy morphemes to learn: frequent, fixed form and relative position to stem, clear function.  

Do children know grammatical rules?  
- Early word combinations are systematic  
- There are overgeneralization errors  


**Human language processing**  

E.g., "The woman brought the knife into the kitchen tripped." This is a somewhat surprising sentence.  

Human's cognitive state in processing texts.

Identification of sequential input of words are highly incremental. IN other words, humans start processing the words even before hearing the second phonemes of them.  

Structural forgetting effect.

**Cognitive evaluation of NLP systems**  
Topics:  
- Applying methods from psycholinguistics and cognitive science to analyze neural networks.  
- Characterizing complex human behavior around languages as a target for NLP systems.  

Psycholinguistic assessment  
- Treat brain as a blackbox.  
- What syntactic structures are ewasy vs. hard for NN language models? (e.g., Filler-Gap dependencies. Wilcox et al., 2018, 2019) 

**Language evolution and emergence**  
- A recently-emerging exciting problem in NLP. E.g., deep RL models.  